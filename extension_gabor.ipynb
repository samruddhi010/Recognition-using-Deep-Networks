{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extension_gabor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOV4jskswWuo3qmKHIhZHp8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pheonix10101/PRCV_p_5/blob/main/extension_gabor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLJZPAsqjYVt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Project 5: Recognition using Deep Networks\n",
        "\n",
        "Author: Samruddhi Raut\n",
        "\n",
        "This file contain the Gobor extension\n",
        "\"\"\"\n",
        "#Import statements\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "import CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining a Network for gabor**"
      ],
      "metadata": {
        "id": "oipS4ZqWNS1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gabor(nn.Module):\n",
        "    def __init__(self, kernels):\n",
        "        super().__init__()\n",
        "        self.kernels = kernels\n",
        "        self.conv1 = nn.Conv2d(1, 10, (5, 5))\n",
        "        self.conv2 = nn.Conv2d(10, 20, (5, 5))\n",
        "        self.conv2_drop = nn.Dropout2d(0.5)\n",
        "        self.flat1 = nn.Flatten()# flatten operation\n",
        "        self.fc1 = nn.Linear(20 * 4 * 4, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)"
      ],
      "metadata": {
        "id": "PU5jXwsMLRd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**computes a forward pass for the network**"
      ],
      "metadata": {
        "id": "8mUqKBoONs9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "        imgs = []\n",
        "        for i in range(len(x)):\n",
        "            kernels = []\n",
        "            for j in self.kernels:\n",
        "                img = cv2.filter2D(x[i][0].detach().numpy(), -1, j)\n",
        "                H = np.floor(np.array(j.shape) / 2).astype(np.int64)\n",
        "                valid_img = img[H[0]:-H[0], H[1]:-H[1]]\n",
        "                kernels.append(valid_img)\n",
        "            imgs.append(kernels)\n",
        "        x = torch.from_numpy(np.array(imgs))\n",
        "        x = F.relu(F.max_pool2d(x, (2, 2)))  # A max pooling layer with a 2x2 window and a ReLU function applied\n",
        "        x = self.conv2(x)# A convolution layer with 20 5x5 filters\n",
        "        x = self.conv2_drop(x) # A dropout layer with a 0.5 dropout rate (50%)\n",
        "        x = F.relu(F.max_pool2d(x, (2, 2)))   # A max pooling layer with a 2x2 window and a ReLU function applied\n",
        "        x = F.relu(self.fc1(self.flat1(x)))# A flattening operation followed by a fully connected Linear layer with 50 nodes and a ReLU function on theoutput\n",
        "        x = F.log_softmax(self.fc2(x), dim = 1) # A final fully connected Linear layer with 10 nodes and the log_softmax function applied to the output\n",
        "        return x"
      ],
      "metadata": {
        "id": "QfBmk0LsLZiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**building a gabor**"
      ],
      "metadata": {
        "id": "6d3AEMzEOKe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gabor():\n",
        "    filters = []\n",
        "\n",
        "    for theta in np.arange(0, np.pi, np.pi / 2):\n",
        "        for k in range(5):\n",
        "            kern = cv2.getGaborKernel((5, 5), 1.0, theta, np.pi / 2.0, 0.5, 0, ktype = cv2.CV_32F)\n",
        "            kern /= 1.5 * kern.sum()\n",
        "            filters.append(kern)\n",
        "    return filters"
      ],
      "metadata": {
        "id": "L1NOeW0VLaXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining a main function**"
      ],
      "metadata": {
        "id": "CgOAl6vmONFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    kernels = build_gabor()\n",
        "\n",
        "    Gabour_network = Gabor(kernels)\n",
        "    loaded_net_state_dict = torch.load('gabor_neural.pt')\n",
        "    Gabour_network.load_state_dict(loaded_net_state_dict)\n",
        "\n",
        "    train_loader, test_loader = CNN.read_and_print(CNN.BATCH_SIZE_TRAIN, CNN.BATCH_SIZE_TEST,\n",
        "                                                           False)\n",
        "    Gabour_network.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    extracted_examples = []\n",
        "    extracted_pred_ictions = []\n",
        "    extracted_targets = []\n",
        "    with torch.no_grad(): # disable gradient calculation is useful for inference, backward() will not be called in testing\n",
        "        c = 0\n",
        "        for data, target in test_loader:\n",
        "            output = Gabour_network(data)\n",
        "            for i in range(CNN.BATCH_SIZE_TEST):\n",
        "                if c >= 10:\n",
        "                    break\n",
        "                extracted_examples.append(data[i])\n",
        "                extracted_pred_ictions.append(output[i])\n",
        "                extracted_targets.append(target[i])\n",
        "                c += 1\n",
        "            test_loss += F.cross_entropy(output, target, reduction = 'sum').item()\n",
        "            pred_iction = output.data.max(1, keepdim = True)[1]\n",
        "            correct += pred_iction.eq(target.data.view_as(pred_iction)).sum()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest over entire test set: Avg.loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    # run the model through the first 10 examples\n",
        "    examples = torch.stack(extracted_examples)\n",
        "    pred_ictions = torch.stack(extracted_pred_ictions)\n",
        "    targets = torch.stack(extracted_targets)\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        output = Gabour_network(examples)\n",
        "        print(\"output: \", end = \"\")\n",
        "        test_loss += F.cross_entropy(output, targets, reduction = 'sum').item()\n",
        "        pred_iction = output.data.max(1, keepdim = True)[1]\n",
        "        correct += pred_iction.eq(targets.data.view_as(pred_iction)).sum()\n",
        "    test_loss /= len(examples)\n",
        "\n",
        "    print('\\nTest over entire test set: Avg.loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(examples), 100. * correct / len(examples)))\n",
        "\n",
        "    for i in range(len(examples)):\n",
        "        print(f\"\\nExample {i + 1}: \\nOutput values :\", end = \"\")\n",
        "        print(['%.2f' % t for t in pred_ictions[i]])\n",
        "        print(\"Predicted label: %d\" % [t for t in pred_ictions[i]].index(max(pred_ictions[i])))\n",
        "        print(\"Correct label: %d\" % targets[i].item())\n",
        "\n",
        "    fig = plt.figure()\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.tight_layout()\n",
        "        plt.imshow(examples[i][0], cmap = 'gray', interpolation = 'none')\n",
        "        plt.title(\"Prediction: %d\" % ([t for t in pred_ictions[i]].index(max(pred_ictions[i]))))\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "1kDq7KHqRGOm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}