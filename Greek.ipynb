{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Greek.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMh8nafzW+cGRCgSBFx6ryq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pheonix10101/PRCV_p_5/blob/main/Greek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Project 5: Recognition using Deep Networks\n",
        "\n",
        "Author: Samruddhi Raut\n",
        "\n",
        "This file contain following tasks\n",
        "\n",
        "Task 3:Use the trained network as an embedding space for images of written symbols.\n",
        "\n",
        "A:Create a greek symbol data set\n",
        "\n",
        "B:Create a truncated model\n",
        "\n",
        "C:Project the greek symbols into the embedding space\n",
        "\n",
        "D:Compute distances in the embedding space\n",
        "\n",
        "E:Create your own greek symbol data\n",
        "\n",
        "\"\"\"\n",
        "#Import_statement\n",
        "import torch\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "import CNN\n",
        "import new_network\n"
      ],
      "metadata": {
        "id": "BH0p3CIlMsGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining** Submodel"
      ],
      "metadata": {
        "id": "v2jeZJf2cGde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "CONV_FILTER_SIZE = [5, 10]\n",
        "DROPOUT_RATE = [0.5,0.8 ]\n",
        "KNN_INPUT_FILENAME = '/content/greek_hand write'\n",
        "\n",
        "\n",
        "class Submodel(CNN.NeuralNetwork):\n",
        "    def __init__(self, conv_filter = 5, dropout_rate = 0.5):\n",
        "        super().__init__(conv_filter, dropout_rate)\n",
        "\n",
        "    def forward(self, x):# override the forward method\n",
        "        x = self.conv1(x)# A convolution layer with 10 5x5 filters\n",
        "        x = F.relu(F.max_pool2d(x, (2, 2)))# A max pooling layer with a 2x2 window and a ReLU function applied\n",
        "        x = self.conv2(x)# A convolution layer with 20 5x5 filters\n",
        "        x = self.conv2_drop(x)# A dropout layer with a 0.5 dropout rate (50%)\n",
        "        x = F.relu(F.max_pool2d(x, (2, 2)))# A max pooling layer with a 2x2 window and a ReLU function applied\n",
        "        x = F.relu(self.fc1(self.flat1(x)))# A flattening operation followed by a fully connected Linear layer with 50 nodes and a ReLU function on the\n",
        "        return x"
      ],
      "metadata": {
        "id": "6WY-rM7JM2Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining SSD**"
      ],
      "metadata": {
        "id": "4P9sYnbdPqsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ssd(a, b):\n",
        "    d = np.sum((a - b) ** 2)\n",
        "    return d"
      ],
      "metadata": {
        "id": "_TPfkT3CM9mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving files to CSV**\n",
        "\n",
        "saving intensity files\n",
        "saving labels file"
      ],
      "metadata": {
        "id": "yu167GWpP1Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv():\n",
        "   \n",
        "    #Save the intensity values and their labels into two csv files\n",
        "    \n",
        "    #intensity values\n",
        "    with open('/content/G_intensity.csv', 'w', encoding = 'UTF8', newline = '') as f:\n",
        "        writer = csv.writer(f)\n",
        "        header = ['Filename', 'Intensity']\n",
        "        writer.writerow(header)\n",
        "        for filename in os.listdir('/content/greek-1'):\n",
        "            image = Image.open(os.path.join('/content/greek-1', filename))\n",
        "            writer.writerow([filename, np.array(image)])\n",
        "\n",
        "    #label values\n",
        "    with open('/content/G_labels.csv', 'w', encoding = 'UTF8', newline = '') as f:\n",
        "        writer = csv.writer(f)\n",
        "        header = ['Filename', 'Label']\n",
        "        writer.writerow(header)\n",
        "        for filename in os.listdir('/content/greek-1'):\n",
        "            if 'alpha' in filename:\n",
        "                writer.writerow([filename, 0])\n",
        "            elif 'beta' in filename:\n",
        "                writer.writerow([filename, 1])\n",
        "            elif 'gamma' in filename:\n",
        "                writer.writerow([filename, 2])\n",
        "            elif 'eta' in filename:\n",
        "                writer.writerow([filename, 3])\n",
        "            elif 'phi' in filename:\n",
        "                writer.writerow([filename, 4])\n",
        "\n",
        "    dataset_greek_values = new_network.NumDraw_Data(annotations_file = '/content/G_labels.csv',\n",
        "                                                   img_dir = '/content/greek-1')\n",
        "    dataloader_greek_values = DataLoader(dataset = dataset_greek_values,\n",
        "                                  batch_size = CNN.BATCH_SIZE_TEST,\n",
        "                                  shuffle = False,\n",
        "                                  num_workers = 4)\n",
        "\n",
        "    return dataset_greek_values, dataloader_greek_values"
      ],
      "metadata": {
        "id": "D5SmPwylNBXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply the truncated network to the greek symbols (read from the CSV file) to get a set of 27 50 element vectors**\n",
        "\n",
        "param greek_submodel:      submodel build from loaded network\n",
        ".param dataloader_greek_values:    dataloader of greek dataset\n",
        "return  overall shape of outputs, and labels\n",
        "  "
      ],
      "metadata": {
        "id": "WTFlzmXhQ5Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding(greek_submodel, dataloader_greek_values):\n",
        "  \n",
        "    greek_submodel.eval()\n",
        "    results = []\n",
        "    targets = []\n",
        "    b = 0\n",
        "    for data, target in dataloader_greek_values:\n",
        "        output = greek_submodel(data)\n",
        "        print(\"\\nBatch %d:\" % b)\n",
        "        print(\"Input batch size: \", end = \"\")\n",
        "        print(data.shape)\n",
        "        print(\"Apply the submodel with 50-node dense layer to the data, \"\n",
        "              \"we have the returned output with the shape of: \", end = \"\")\n",
        "        print(output.shape)\n",
        "        b += 1\n",
        "# make sure no matter what the batch size is, the results will always keep all outputs, in this case,\n",
        "# shape of 27 * 50; and targets will have the corresponding labels\n",
        "        for i in range(len(output)):\n",
        "            results.append(output[i])\n",
        "            targets.append(target[i])\n",
        "    print(\"\\nShape of the output nodes from the submodel: \", end = \"\")\n",
        "    print(torch.stack(results).shape)\n",
        "    print(\"Number of the labels: \", end = \"\")\n",
        "    print(torch.stack(targets).shape)\n",
        "\n",
        "    return results, targets"
      ],
      "metadata": {
        "id": "m-Wea2iBNLj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract 3 greek symbols**\n",
        "\n",
        "param results: outputs from submodel\n",
        ",param targets: labels corresponding to the results\n",
        ",return selected alpha, beta, and gamma"
      ],
      "metadata": {
        "id": "0C8Qe75rTAJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_greek_letter(results, targets):\n",
        "    alpha = []\n",
        "    beta = []\n",
        "    gamma = []\n",
        "    label = 99\n",
        "    for i in range(len(results)):\n",
        "        if targets[i].detach().numpy() != label and targets[i].detach().numpy() == 0:\n",
        "            for j in results[i].detach().numpy():\n",
        "                alpha.append(j)\n",
        "            label = targets[i].detach().numpy()\n",
        "        elif targets[i].detach().numpy() != label and targets[i].detach().numpy() == 1:\n",
        "            for j in results[i].detach().numpy():\n",
        "                beta.append(j)\n",
        "            label = targets[i].detach().numpy()\n",
        "        elif targets[i].detach().numpy() != label and targets[i].detach().numpy() == 2:\n",
        "            for j in results[i].detach().numpy():\n",
        "                gamma.append(j)\n",
        "            label = targets[i].detach().numpy()\n",
        "\n",
        "    return alpha, beta, gamma"
      ],
      "metadata": {
        "id": "TiQXmRtGNQr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate the label of the given data by assigning it the label of knn's label**\n",
        "param k:       k nearest neighbors\n",
        "param results: outputs from submodel\n",
        "param targets: labels corresponding to the results\n",
        "param a:       given example"
      ],
      "metadata": {
        "id": "Un88DgJwTcsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knn(k, results, targets, a):\n",
        "   \n",
        "    sum_alpha = []\n",
        "    sum_beta = []\n",
        "    sum_gamma = []\n",
        "    sum_eta = []\n",
        "    sum_phi = []\n",
        "    for i in range(len(results)):\n",
        "        d = ssd(np.array(a), results[i].detach().numpy())\n",
        "        print(\"%.2f\" % d, end = \" \")\n",
        "        if targets[i].detach().numpy() == 0:\n",
        "            sum_alpha.append(d)\n",
        "        elif targets[i].detach().numpy() == 1:\n",
        "            sum_beta.append(d)\n",
        "        elif targets[i].detach().numpy() == 2:\n",
        "            sum_gamma.append(d)\n",
        "        elif targets[i].detach().numpy() == 3:\n",
        "            sum_eta.append(d)\n",
        "        elif targets[i].detach().numpy() == 4:\n",
        "            sum_phi.append(d)\n",
        "    sum_alpha.sort()\n",
        "    sum_beta.sort()\n",
        "    sum_gamma.sort()\n",
        "    sum_eta.sort()\n",
        "    sum_phi.sort()\n",
        "    print(\"\\nTotal distance to top kth label alpha: %.2f\" % sum(sum_alpha[: k]))\n",
        "    print(\"Total distance to top kth label beta: %.2f\" % sum(sum_beta[: k]))\n",
        "    print(\"Total distance to top kth label gamma: %.2f\" % sum(sum_gamma[: k]))\n",
        "    print(\"Total distance to top kth label eta: %.2f\" % sum(sum_eta[: k]))\n",
        "    print(\"Total distance to top kth label phi: %.2f\" % sum(sum_phi[: k]))\n",
        "\n",
        "    prediction = min(sum(sum_alpha[: k]), sum(sum_beta[: k]),\n",
        "               sum(sum_gamma[: k]), sum(sum_eta[: k]),\n",
        "               sum(sum_phi[: k]))\n",
        "\n",
        "    if prediction == sum(sum_alpha[: k]):\n",
        "        return \"alpha\"\n",
        "    elif prediction == sum(sum_beta[: k]):\n",
        "        return \"beta\"\n",
        "    elif prediction == sum(sum_gamma[: k]):\n",
        "        return \"gamma\"\n",
        "    elif prediction == sum(sum_eta[: k]):\n",
        "        return \"eta\"\n",
        "    elif prediction == sum(sum_phi[: k]):\n",
        "        return \"phi\"\n",
        "    else:\n",
        "        return \"unknown\""
      ],
      "metadata": {
        "id": "6tvPwnnXNCMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Calculate the average distance from a given example to all of other examples\n",
        "param results: outputs from submodel,param targets: labels corresponding to the results,param a:given example\n",
        "  "
      ],
      "metadata": {
        "id": "kS0hOoHia14F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_avg(results, targets, a):\n",
        "    \n",
        "    avg_alpha = []\n",
        "    avg_beta = []\n",
        "    avg_gamma = []\n",
        "    avg_eta = []\n",
        "    avg_phi = []\n",
        "    for i in range(len(results)):\n",
        "        d = ssd(np.array(a), results[i].detach().numpy())\n",
        "        print(\"%.2f\" % d, end = \" \")\n",
        "        if d == 0:\n",
        "            continue\n",
        "        if targets[i].detach().numpy() == 0:\n",
        "            avg_alpha.append(d)\n",
        "        elif targets[i].detach().numpy() == 1:\n",
        "            avg_beta.append(d)\n",
        "        elif targets[i].detach().numpy() == 2:\n",
        "            avg_gamma.append(d)\n",
        "        elif targets[i].detach().numpy() == 3:\n",
        "            avg_eta.append(d)\n",
        "        elif targets[i].detach().numpy() == 4:\n",
        "            avg_phi.append(d)\n",
        "\n",
        "    print(\"\\nAverage distance to label alpha: %.2f\" % (sum(avg_alpha) / len(avg_alpha)))\n",
        "    print(\"Average distance to label beta: %.2f\" % (sum(avg_beta) / len(avg_beta)))\n",
        "    print(\"Average distance to label gamma: %.2f\" % (sum(avg_gamma) / len(avg_gamma)))\n",
        "    print(\"Average distance to label eta: %.2f\" % (sum(avg_eta) / len(avg_eta)))\n",
        "    print(\"Average distance to label phi: %.2f\" % (sum(avg_phi) / len(avg_phi)))\n",
        "\n",
        "    prediction = min((sum(avg_alpha) / len(avg_alpha)), (sum(avg_beta) / len(avg_beta)),\n",
        "               (sum(avg_gamma) / len(avg_gamma)), (sum(avg_eta) / len(avg_eta)),\n",
        "               (sum(avg_phi) / len(avg_phi)))\n",
        "\n",
        "    if prediction == (sum(avg_alpha) / len(avg_alpha)):\n",
        "        return \"alpha\"\n",
        "    elif prediction == (sum(avg_beta) / len(avg_beta)):\n",
        "        return \"beta\"\n",
        "    elif prediction == (sum(avg_gamma) / len(avg_gamma)):\n",
        "        return \"gamma\"\n",
        "    elif prediction == (sum(avg_eta) / len(avg_eta)):\n",
        "        return \"eta\"\n",
        "    elif prediction == (sum(avg_phi) / len(avg_phi)):\n",
        "        return \"phi\"\n",
        "    else:\n",
        "        return \"unknown\""
      ],
      "metadata": {
        "id": "7mqYr7qaNYOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating hand written greek letter* **"
      ],
      "metadata": {
        "id": "Wl-dPSWqYqTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_greek_written(greek_submodel, results, targets, is_knn, s, dr):\n",
        "    hand_write_greeks = new_network.NumDraw_Data(annotations_file = '/content/greek_hand write.csv',\n",
        "                                                       img_dir = '/content/greek_hand write')\n",
        "    hand_write_greeks_dataloader = DataLoader(dataset = hand_write_greeks,\n",
        "                                              batch_size = CNN.BATCH_SIZE_TEST,\n",
        "                                              shuffle = False,\n",
        "                                              num_workers = 4)\n",
        "\n",
        "    print(\"\\nBuilding embedding space for hand write greeks\")\n",
        "    hand_write_results, hand_write_targets = embedding(greek_submodel, hand_write_greeks_dataloader)\n",
        "\n",
        "    imgs = []\n",
        "    for data, target in hand_write_greeks_dataloader:\n",
        "        imgs.append(data)\n",
        "\n",
        "    if (len(hand_write_results) / CNN.BATCH_SIZE_TEST).is_integer():\n",
        "        batch_num = len(hand_write_results) / CNN.BATCH_SIZE_TEST\n",
        "    else:\n",
        "        batch_num = int(len(hand_write_results) / CNN.BATCH_SIZE_TEST) + 1\n",
        "\n",
        "    predictions = []\n",
        "    for i in range(len(hand_write_results)):\n",
        "        print(\"\\nDistance from selected image %d to others: \" % i)\n",
        "        if is_knn:\n",
        "            predictions.append(knn(7, results, targets, hand_write_results[i].detach().numpy()))\n",
        "        else:\n",
        "            predictions.append(calculate_avg(results, targets, hand_write_results[i].detach().numpy()))\n",
        "\n",
        "    i = 0\n",
        "    c = 0\n",
        "    fig = plt.figure()\n",
        "    while i < batch_num:\n",
        "        for j in range(CNN.BATCH_SIZE_TEST):\n",
        "            if c >= len(hand_write_results):\n",
        "                i = batch_num\n",
        "                break\n",
        "            plt.subplot(3, 5, c + 1)\n",
        "            plt.tight_layout()\n",
        "            plt.imshow(imgs[i][j][0], cmap = 'gray', interpolation = 'none')\n",
        "            if is_knn:\n",
        "                plt.title(\"s{}dr{}kprediction:\\n{}\".format(s, dr, predictions[c]))\n",
        "            else:\n",
        "                plt.title(\"s{}dr{}prediction:\\n{}\".format(s, dr, predictions[c]))\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "CP6SodUvNc6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**prediction_by_knn**"
      ],
      "metadata": {
        "id": "wy5M8L6TbtWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_by_knn(filename, k, greek_submodel, results, targets):\n",
        "    image = read_image(filename).float()\n",
        "    image = image[None, :]\n",
        "    output = greek_submodel(image)\n",
        "    print('\\nDistance from input image to greek database:')\n",
        "    prediction = knn(k, results, targets, output[0].detach().numpy())\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(image[0][0], cmap = 'gray', interpolation = 'none')\n",
        "    plt.title(\"Prediction: %s\" % prediction)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "bH7kJLIGNias"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define main function**"
      ],
      "metadata": {
        "id": "sm-xlxdiZGbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # load the model\n",
        "    loaded_net = CNN.NeuralNetwork()\n",
        "    loaded_net_state_dict = torch.load('samruddhi_neural.pt')\n",
        "    loaded_net.load_state_dict(loaded_net_state_dict)\n",
        "    loaded_net.eval()\n",
        "\n",
        "    dataset_greek_values, dataloader_greek_values = save_to_csv()\n",
        "\n",
        "    for i in CONV_FILTER_SIZE:\n",
        "        for j in DROPOUT_RATE:\n",
        "            print(\"For kernel size = {} and drop rate = {}\\n\".format(i, j))\n",
        "\n",
        "            greek_submodel = Submodel(i, j)\n",
        "\n",
        "            print(\"Building embedding space of greek-1\")\n",
        "            results, targets = embedding(greek_submodel, dataloader_greek_values)\n",
        "            print(\"Embedding space of greek-1 built\")\n",
        "\n",
        "            alpha, beta, gamma = get_greek_letter(results, targets)\n",
        "\n",
        "            print(\"\\nDistance from selected alpha to others: \")\n",
        "            calculate_avg(results, targets, alpha)\n",
        "            print(\"\\nDistance from selected beta to others: \")\n",
        "            calculate_avg(results, targets, beta)\n",
        "            print(\"\\nDistance from selected gamma to others: \")\n",
        "            calculate_avg(results, targets, gamma)\n",
        "            print('\\n')\n",
        "\n",
        "            evaluate_greek_written(greek_submodel, results, targets, False, i, j)\n",
        "            evaluate_greek_written(greek_submodel, results, targets, True, i, j)\n",
        "\n",
        "    greek_submodel = Submodel()\n",
        "    results, targets = embedding(greek_submodel, dataloader_greek_values)\n",
        "    prediction_by_knn(KNN_INPUT_FILENAME, 3, greek_submodel, results, targets)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "469XcjeVeVG3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}