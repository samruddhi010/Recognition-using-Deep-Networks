{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPpgJ0qtQGl/omzrzuXVyXy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pheonix10101/PRCV_p_5/blob/main/Task_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-AbZZzbfPvL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Project 5: Recognition using Deep Networks\n",
        "\n",
        "Author: Samruddhi Raut\n",
        "\n",
        "This file contain following tasks\n",
        "\n",
        "Task 4:\n",
        "\n",
        "Design your own experiment\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#Import statement\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed\n",
        "torch.manual_seed(42)\n",
        "cudnn.enabled = False\n",
        "\n",
        "BATCH_SIZE_TRAIN = [64,128,192]\n",
        "BATCH_SIZE_TEST = 1000\n",
        "EPOCHS = [5, 7, 9]\n",
        "LEARNING_RATE = 0.05\n",
        "MOMENTUM = 0.5\n",
        "CONV_FILTER_SIZE = [5,10,15]\n",
        "DROPOUT_RATE = [0.5]\n",
        "LOG_INTERVAL = 10"
      ],
      "metadata": {
        "id": "8DPCkH2jfkya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining a variation class**"
      ],
      "metadata": {
        "id": "KEPXVAI8gYjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Variation(nn.Module):\n",
        "    def __init__(self, conv_filter_size, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, (conv_filter_size, conv_filter_size))\n",
        "        self.conv2 = nn.Conv2d(10, 20, (conv_filter_size, conv_filter_size))\n",
        "        self.conv2_drop = nn.Dropout2d(dropout_rate)\n",
        "        self.flat1 = nn.Flatten()\n",
        "        outer = conv_filter_size // 2\n",
        "        size = ((28 - 2 * outer) // 2 - 2 * outer) // 2\n",
        "        self.fc1 = nn.Linear(20 * size * size, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)# A convolution layer with 10 5x5 filters\n",
        "        x = F.relu(F.max_pool2d(x, (2, 2)))# A max pooling layer with a 2x2 window and a ReLU function applied\n",
        "        x = self.conv2(x)# A convolution layer with 20 5x5 filters\n",
        "        x = self.conv2_drop(x)# A dropout layer with a 0.5 dropout rate (50%)\n",
        "        x = F.relu(F.max_pool2d(x, (2, 2))) # A max pooling layer with a 2x2 window and a ReLU function applied\n",
        "        x = F.relu(self.fc1(self.flat1(x))) # A flattening operation followed by a fully connected Linear layer with 50 nodes and a ReLU function on the output\n",
        "        x = F.log_softmax(self.fc2(x), dim = 1) # A final fully connected Linear layer with 10 nodes and the log_softmax function applied to the output\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "iwusLrmWfo9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(batch_size_train):\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    train_mnist = datasets.MNIST('mnist',\n",
        "                                 train = True,\n",
        "                                 download = True,\n",
        "                                 transform = transform\n",
        "                                 )\n",
        "    test_mnist = datasets.MNIST('mnist',\n",
        "                                train = False,\n",
        "                                download = True,\n",
        "                                transform = transform\n",
        "                                )\n",
        "    train_loader = DataLoader(dataset = train_mnist,\n",
        "                              batch_size = batch_size_train,\n",
        "                              shuffle = True,\n",
        "                              num_workers = 4)\n",
        "    test_loader = DataLoader(dataset = test_mnist,\n",
        "                             batch_size = BATCH_SIZE_TEST,\n",
        "                             shuffle = True,\n",
        "                             num_workers = 4)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ],
      "metadata": {
        "id": "kW-7mvqAfsP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**defening train_and_test**\n",
        "\n",
        "train the model on training set epochs# times\n",
        "    :param train_loader:    dataloader of training set\n",
        "    :param test_loader:     dataloader of testing set\n",
        "    :param batch_size:      batch_size of training\n",
        "    :param epochs:          times complete pass through the training data\n",
        "    :param conv_filter_size:size of conv filter\n",
        "    :param dropout_rate:    dropout rate\n",
        "    :param learning_rate:   learning rate\n",
        "    :param momentum:        refers to inertia\n",
        "    :param log_interval:    interval between every two recordings of loss\n",
        "    "
      ],
      "metadata": {
        "id": "VbHOdk37hJVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test(train_loader, test_loader, batch_size, epochs, conv_filter_size, dropout_rate,\n",
        "                   learning_rate, momentum, log_interval):\n",
        "    \n",
        "    # initialize model and optimizer\n",
        "    network = Variation(conv_filter_size, dropout_rate)\n",
        "    optimizer = optim.SGD(network.parameters(), lr = learning_rate, momentum = momentum)\n",
        "    train_losses = []\n",
        "    train_counter = []\n",
        "    test_losses_training_set = []\n",
        "    test_losses = []\n",
        "    test_counter = [i * len(train_loader.dataset) for i in range(epochs + 1)]\n",
        "\n",
        "    CNN.test_network(network, train_loader, test_losses_training_set, 0, False)\n",
        "    CNN.test_network(network, test_loader, test_losses, 0, True)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        CNN.train_network(network, train_loader, epoch, optimizer, log_interval, train_losses,\n",
        "                                  train_counter, False)\n",
        "        CNN.test_network(network, train_loader, test_losses_training_set, epoch, False)\n",
        "        CNN.test_network(network, test_loader, test_losses, epoch, True)\n",
        "\n",
        "    \n",
        "    fig = plt.figure()\n",
        "    plt.plot(train_counter, train_losses, color = 'green')\n",
        "    plt.scatter(test_counter, test_losses, color = 'red', alpha = 1)\n",
        "    plt.legend(['Train Loss', 'Test Loss on training set'], loc = 'upper right')\n",
        "    plt.title('Batch size: {} Epoch #: {} Conv filter size: {} Dropout rate: {}'.format(batch_size,\n",
        "                                                                                        epochs,\n",
        "                                                                                        conv_filter_size,\n",
        "                                                                                        dropout_rate))\n",
        "    plt.xlabel('number of training examples seen')\n",
        "    plt.ylabel('negative log likelihood loss')\n",
        "   \n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "AHiahaFXfxBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a main function"
      ],
      "metadata": {
        "id": "vMN4Bb1chHC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    c = 1\n",
        "    for i in BATCH_SIZE_TRAIN:\n",
        "        train_loader, test_loader = get_loader(i)\n",
        "        for j in EPOCHS:\n",
        "            for p in CONV_FILTER_SIZE:\n",
        "                for q in DROPOUT_RATE:\n",
        "                    start = time.time()\n",
        "                    print('Training and evaluating for variation %d:' % c)\n",
        "                    print('\\tBatch size: {}'.format(i))\n",
        "                    print('\\tEpoch #: {}'.format(j))\n",
        "                    print('\\tConv filter size: {}'.format(p))\n",
        "                    print('\\tDropout rate: {}\\n'.format(q))\n",
        "                    train_and_test(train_loader, test_loader, i, j, p, q, LEARNING_RATE, MOMENTUM, LOG_INTERVAL)\n",
        "                    end = time.time()\n",
        "                    print(\"Time cost: %.2fs\" % (end - start))\n",
        "                    c += 1\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "OWF0gXHwjGDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}