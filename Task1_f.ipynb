{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task1_f.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkSuJKsVqAP5doWakllUBA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pheonix10101/PRCV_p_5/blob/main/Task1_f.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tXMTTy6wevBf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Project 5: Recognition using Deep Networks\n",
        "\n",
        "Author: Samruddhi Raut\n",
        "\n",
        "This file contain following tasks\n",
        "Task 1\n",
        "F:Read the network and run it on the test set\n",
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.io import read_image\n",
        "from matplotlib import pyplot as plt\n",
        "import CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)#In order to make your code repeatable, set the random seed for the torch package"
      ],
      "metadata": {
        "id": "WGDH1yKnl_Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining main function**"
      ],
      "metadata": {
        "id": "JuE7ICdafjcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    loaded_net = CNN.NeuralNetwork()\n",
        "    loaded_net_state_dict = torch.load('/content/samruddhi_neural.pt')\n",
        "    loaded_net.load_state_dict(loaded_net_state_dict)\n",
        "  \n",
        "    train_loader, test_loader = CNN.read_and_print(CNN.BATCH_SIZE_TRAIN, CNN.BATCH_SIZE_TEST,\n",
        "                                                           False)\n",
        "# check if the model read from file is correct\n",
        "# set model to eval()\n",
        "    loaded_net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    extracted_examples = []\n",
        "    extracted_preds = []\n",
        "    extracted_targets = []\n",
        "   \n",
        "    with torch.no_grad(): # disable gradient calculation is useful for inference, backward() will not be called in testing\n",
        "        c = 0\n",
        "        for data, target in test_loader:\n",
        "            output = loaded_net(data)\n",
        "            for i in range(CNN.BATCH_SIZE_TEST):\n",
        "                if c >= 10:\n",
        "                    break\n",
        "                extracted_examples.append(data[i])\n",
        "                extracted_preds.append(output[i])\n",
        "                extracted_targets.append(target[i])\n",
        "                c += 1\n",
        "            test_loss += F.cross_entropy(output, target, reduction = 'sum').item()\n",
        "            pred = output.data.max(1, keepdim = True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest over entire test set: Avg.loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "# run the model through the first 10 examples\n",
        "    examples = torch.stack(extracted_examples)\n",
        "    preds = torch.stack(extracted_preds)\n",
        "    targets = torch.stack(extracted_targets)\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        output = loaded_net(examples)\n",
        "        print(\"output: \", end = \"\")\n",
        "        test_loss += F.cross_entropy(output, targets, reduction = 'sum').item()\n",
        "        pred = output.data.max(1, keepdim = True)[1]\n",
        "        correct += pred.eq(targets.data.view_as(pred)).sum()\n",
        "    test_loss /= len(examples)\n",
        "\n",
        "    print('\\nTest over entire test set: Avg.loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(examples), 100. * correct / len(examples)))\n",
        "\n",
        "    for i in range(len(examples)):\n",
        "        print(f\"\\nExample {i + 1}: \\nOutput values :\", end = \"\")\n",
        "        print(['%.2f' % t for t in preds[i]])\n",
        "        print(\"Predicted label: %d\" % [t for t in preds[i]].index(max(preds[i])))\n",
        "        print(\"Correct label: %d\" % targets[i].item())\n",
        "\n",
        "    fig = plt.figure()\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.tight_layout()\n",
        "        plt.imshow(examples[i][0], cmap = 'gray', interpolation = 'none')\n",
        "        plt.title(\"Prediction: %d\" % ([t for t in preds[i]].index(max(preds[i]))))\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    fig.show()\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "03JnX3zZl7bP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}